{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from scipy.special import softmax\n",
    "import requests\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import CLIPImageProcessor, CLIPVisionModel\n",
    "from src.models.llava_1 import LlavaMPTForCausalLM, LlavaLlamaForCausalLM, conv_templates, SeparatorStyle\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = \"<im_end>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_model_name(model_path):\n",
    "    # get model name\n",
    "    if model_path.endswith(\"/\"):\n",
    "        model_path = model_path[:-1]\n",
    "    model_paths = model_path.split(\"/\")\n",
    "    if model_paths[-1].startswith('checkpoint-'):\n",
    "        model_name = model_paths[-2] + \"_\" + model_paths[-1]\n",
    "    else:\n",
    "        model_name = model_paths[-1]\n",
    "    \n",
    "    return model_name\n",
    "\n",
    "\n",
    "def get_conv(model_name):\n",
    "    if \"llava\" in model_name.lower():\n",
    "        if \"v1\" in model_name.lower():\n",
    "            template_name = \"llava_v1\"\n",
    "        elif \"mpt\" in model_name.lower():\n",
    "            template_name = \"mpt_multimodal\"\n",
    "        else:\n",
    "            template_name = \"multimodal\"\n",
    "    elif \"mpt\" in model_name:\n",
    "        template_name = \"mpt_text\"\n",
    "    elif \"koala\" in model_name: # Hardcode the condition\n",
    "        template_name = \"bair_v1\"\n",
    "    elif \"v1\" in model_name:    # vicuna v1_1/v1_2\n",
    "        template_name = \"vicuna_v1_1\"\n",
    "    else:\n",
    "        template_name = \"v1\"\n",
    "    return conv_templates[template_name].copy()\n",
    "\n",
    "\n",
    "def load_model(model_path, model_name, dtype=torch.float16, device='cpu'):\n",
    "    # get tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if 'llava' in model_name.lower():\n",
    "        if 'mpt' in model_name.lower():\n",
    "            model = LlavaMPTForCausalLM.from_pretrained(model_path, torch_dtype=dtype, low_cpu_mem_usage=True)\n",
    "        else:\n",
    "            model = LlavaLlamaForCausalLM.from_pretrained(model_path, torch_dtype=dtype, low_cpu_mem_usage=True)\n",
    "    elif 'mpt' in model_name.lower():\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=dtype, low_cpu_mem_usage=True, trust_remote_code=True)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=dtype, low_cpu_mem_usage=True)\n",
    "\n",
    "    # get image processor\n",
    "    image_processor = None\n",
    "    if 'llava' in model_name.lower():\n",
    "        image_processor = CLIPImageProcessor.from_pretrained(model.config.mm_vision_tower, torch_dtype=dtype)\n",
    "\n",
    "        mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n",
    "        tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n",
    "        if mm_use_im_start_end:\n",
    "            tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n",
    "\n",
    "        vision_tower = model.get_model().vision_tower[0]\n",
    "        if vision_tower.device.type == 'meta':\n",
    "            vision_tower = CLIPVisionModel.from_pretrained(vision_tower.config._name_or_path, torch_dtype=dtype, low_cpu_mem_usage=True).to(device=device)\n",
    "            model.get_model().vision_tower[0] = vision_tower\n",
    "        else:\n",
    "            vision_tower.to(device=device, dtype=dtype)\n",
    "        \n",
    "        vision_config = vision_tower.config\n",
    "        vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]\n",
    "        vision_config.use_im_start_end = mm_use_im_start_end\n",
    "        if mm_use_im_start_end:\n",
    "            vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\n",
    "\n",
    "    if hasattr(model.config, \"max_sequence_length\"):\n",
    "        context_len = model.config.max_sequence_length\n",
    "    else:\n",
    "        context_len = 2048\n",
    "\n",
    "    model.to(device=device)\n",
    "\n",
    "    return tokenizer, model, image_processor, context_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose(\n",
    "    [\n",
    "        # transforms.Resize(\n",
    "        #     224, interpolation=transforms.InterpolationMode.BICUBIC\n",
    "        # ),\n",
    "        # transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(\n",
    "        #     mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "        #     std=(0.26862954, 0.26130258, 0.27577711),\n",
    "        # ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "class LLaVA():\n",
    "    def __init__(self, model_type=\"llava\", device=\"cuda\"):\n",
    "        model_path=\"liuhaotian/LLaVA-Lightning-MPT-7B-preview\"\n",
    "        model_name = get_model_name(model_path)\n",
    "        self.tokenizer, self.model, self.image_processor, self.context_len = load_model(model_path, model_name)\n",
    "        self.conv = get_conv(model_name)\n",
    "        self.image_process_mode = \"Resize\" # Crop, Resize, Pad\n",
    "        self.dtype = torch.float16\n",
    "        self.device = device\n",
    "        self.model_type = model_type\n",
    "        vision_tower = self.model.get_model().vision_tower[0]\n",
    "        vision_tower.to(device=self.device, dtype=self.dtype)\n",
    "        self.model.to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "    \n",
    "    def ask(self, image_path, question):\n",
    "        imgs = [Image.open(image_path).convert('RGB')]\n",
    "        imgs = [image_transform(x) for x in imgs]\n",
    "        # image = torch.stack(imgs, dim=0).to(self.device)  \n",
    "        image = imgs.unsqueeze(0).to(self.device) \n",
    "        conv = self.conv.copy()\n",
    "        text = question + '\\n<image>'\n",
    "        text = (text, image, self.image_process_mode)\n",
    "        conv.append_message(conv.roles[0], text)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompt = conv.get_prompt()\n",
    "        stop_str = conv.sep if conv.sep_style in [SeparatorStyle.SINGLE, SeparatorStyle.MPT] else conv.sep2\n",
    "        output = self.do_generate(prompt, image, stop_str=stop_str, dtype=self.dtype)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def do_generate(self, prompt, image, dtype=torch.float16, temperature=0.2, max_new_tokens=512, stop_str=None, keep_aspect_ratio=False):\n",
    "        images = [image]\n",
    "        assert len(images) == prompt.count(DEFAULT_IMAGE_TOKEN), \"Number of images does not match number of <image> tokens in prompt\"\n",
    "\n",
    "        if keep_aspect_ratio:\n",
    "            new_images = []\n",
    "            for image_idx, image in enumerate(images):\n",
    "                max_hw, min_hw = max(image.size), min(image.size)\n",
    "                aspect_ratio = max_hw / min_hw\n",
    "                max_len, min_len = 448, 224\n",
    "                shortest_edge = int(min(max_len / aspect_ratio, min_len))\n",
    "                image = self.image_processor.preprocess(image, return_tensors='pt', do_center_crop=False, size={\"shortest_edge\": shortest_edge})['pixel_values'][0]\n",
    "                new_images.append(image.to(self.model.device, dtype=dtype))\n",
    "                # replace the image token with the image patch token in the prompt (each occurrence)\n",
    "                cur_token_len = (image.shape[1]//14) * (image.shape[2]//14)\n",
    "                replace_token = DEFAULT_IMAGE_PATCH_TOKEN * cur_token_len\n",
    "                if getattr(self.model.config, 'mm_use_im_start_end', False):\n",
    "                    replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n",
    "                prompt = prompt.replace(DEFAULT_IMAGE_TOKEN, replace_token, 1)\n",
    "            images = new_images\n",
    "        else:\n",
    "            images = self.image_processor(images, return_tensors='pt')['pixel_values']\n",
    "            images = images.to(self.model.device, dtype=dtype)\n",
    "            replace_token = DEFAULT_IMAGE_PATCH_TOKEN * 256    # HACK: 256 is the max image token length hacked\n",
    "            if getattr(self.model.config, 'mm_use_im_start_end', False):\n",
    "                replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n",
    "            prompt = prompt.replace(DEFAULT_IMAGE_TOKEN, replace_token)\n",
    "\n",
    "        stop_idx = None\n",
    "        if stop_str is not None:\n",
    "            stop_idx = self.tokenizer(stop_str).input_ids\n",
    "            if len(stop_idx) == 1:\n",
    "                stop_idx = stop_idx[0]\n",
    "            else:\n",
    "                stop_idx = None\n",
    "\n",
    "        input_ids = self.tokenizer(prompt).input_ids\n",
    "        output_ids = list(input_ids)\n",
    "        pred_ids = []\n",
    "\n",
    "        max_src_len = self.context_len - max_new_tokens - 8\n",
    "        input_ids = input_ids[-max_src_len:]\n",
    "\n",
    "        for i in range(max_new_tokens):\n",
    "            if i == 0:\n",
    "                out = self.model(\n",
    "                    torch.as_tensor([input_ids]).to(self.model.device),\n",
    "                    use_cache=True,\n",
    "                    images=images)\n",
    "                logits = out.logits\n",
    "                past_key_values = out.past_key_values\n",
    "            else:\n",
    "                out = self.model(input_ids=torch.as_tensor([[token]], device=self.model.device),\n",
    "                            use_cache=True,\n",
    "                            attention_mask=torch.ones(1, past_key_values[0][0].shape[-2] + 1, device=self.model.device),\n",
    "                            past_key_values=past_key_values)\n",
    "                logits = out.logits\n",
    "                past_key_values = out.past_key_values\n",
    "\n",
    "            last_token_logits = logits[0][-1]\n",
    "            if temperature < 1e-4:\n",
    "                token = int(torch.argmax(last_token_logits))\n",
    "            else:\n",
    "                probs = torch.softmax(last_token_logits / temperature, dim=-1)\n",
    "                token = int(torch.multinomial(probs, num_samples=1))\n",
    "\n",
    "            output_ids.append(token)\n",
    "            pred_ids.append(token)\n",
    "\n",
    "            if stop_idx is not None and token == stop_idx:\n",
    "                break\n",
    "            elif token == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "            elif i == max_new_tokens - 1:\n",
    "                break\n",
    "   \n",
    "        output = self.tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
    "        if stop_str is not None:\n",
    "            pos = output.rfind(stop_str)\n",
    "            if pos != -1:\n",
    "                output = output[:pos]\n",
    "        \n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_candidates_medical(data):\n",
    "    a,b,c,d = data.get('option_A'), data.get('option_B'), data.get('option_C'), data.get('option_D')\n",
    "    answer_list = [a, b]\n",
    "    if c is not None:\n",
    "        answer_list.append(c)\n",
    "    if d is not None:\n",
    "        answer_list.append(d)\n",
    "    return answer_list\n",
    "\n",
    "\n",
    "def load_prompt(question, idx=4):\n",
    "    prompts = [\"{}\".format(question),\n",
    "               \"{} Answer:\".format(question),\n",
    "               \"{} The answer is\".format(question),\n",
    "               \"Question: {} Answer:\".format(question),\n",
    "               \"Question: {} The answer is\".format(question)\n",
    "               ]\n",
    "    return prompts[idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bytes2PIL(bytes_img):\n",
    "    '''Transform bytes image to PIL.\n",
    "    Args:\n",
    "        bytes_img: Bytes image.\n",
    "    '''\n",
    "    pil_img = Image.open(BytesIO(bytes_img)).convert(\"RGB\")\n",
    "    return pil_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def test(model, dataset=None, model_type='llava', prompt_idx=4, save_path=''):\n",
    "    with open(dataset) as f:\n",
    "        data_all = json.load(f)\n",
    "    cnt = 0\n",
    "    correct = 0\n",
    "    \n",
    "    res = []\n",
    "    for data in data_all:\n",
    "        cnt += 1\n",
    "        question = data['question']\n",
    "        candidates = load_candidates_medical(data)\n",
    "        answer = data['gt_answer']\n",
    "        img_path = data['image_path']  \n",
    "        prefix = load_prompt(question, prompt_idx)\n",
    "        prefix_tokens = model.tokenizer(prefix)\n",
    "        start_loc = len(prefix_tokens.input_ids)\n",
    "        candidate_scores = []  # pred scores of candidates\n",
    "        raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "        images = model.image_processor(raw_image, return_tensors='pt')['pixel_values']\n",
    "        images = images.to(model.model.device, dtype=model.dtype)\n",
    "        for candidate in candidates:\n",
    "            max_new_tokens = 512\n",
    "            prompt = prefix + \" {}.\".format(candidate)\n",
    "            input_ids = model.tokenizer(prompt).input_ids\n",
    "            max_src_len = model.context_len - max_new_tokens - 8\n",
    "            input_ids = input_ids[-max_src_len:]\n",
    "            input_ids = torch.as_tensor([input_ids]).to(model.model.device)\n",
    "            out = model.model(input_ids,use_cache=True,images=images)\n",
    "            logits = out.logits\n",
    "            targets =  input_ids\n",
    "            \n",
    "            prompt = prefix + \" {}.\".format(candidate)\n",
    "            prompt_tokens = model.tokenizer(prompt, return_tensors=\"pt\")\n",
    "            lang_t = prompt_tokens[\"input_ids\"]\n",
    "            \n",
    "            prefix_tokens = model.tokenizer(prefix, return_tensors=\"pt\")  \n",
    "            lang_t1 = prefix_tokens[\"input_ids\"]\n",
    "            lang_diff = lang_t.shape[1] - lang_t1.shape[1]\n",
    "            \n",
    "            \n",
    "            targets[0,:start_loc]=-100\n",
    "            targets[0,start_loc+lang_diff:]=-100\n",
    "            shift_logits = logits[...,:-1,:].contiguous()\n",
    "            shift_labels = targets[...,1:].contiguous()\n",
    "            loss_fct = CrossEntropyLoss(reduction=\"mean\")\n",
    "            loss = loss_fct(shift_logits.view(-1,50282),shift_labels.view(-1))\n",
    "            \n",
    "            candidate_scores.append(loss.item())\n",
    "        data['confidence'] =  str(candidate_scores)\n",
    "        candidate_scores = softmax(np.reciprocal(candidate_scores))\n",
    "        pred = candidates[np.argmax(candidate_scores)]\n",
    "        print(candidates, candidate_scores)\n",
    "        data['model_pred'] = pred\n",
    "        \n",
    "        data['is_correct'] = 'yes' if pred == answer else 'no'\n",
    "        if pred == answer:\n",
    "            correct += 1\n",
    "        res.append(data)\n",
    "        \n",
    "    acc = correct / cnt\n",
    "    print(\"Accuracy: \", acc)\n",
    "        \n",
    "    final_res = {'model_name': model_type, 'dataset_name': dataset, 'correct_precentage': acc, 'pred_dict': res}\n",
    "    \n",
    "    \n",
    "    with open('{}/{}.json'.format(save_path, dataset.replace('/', '_')), 'w') as f:\n",
    "        json.dump(final_res, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'llava'\n",
    "vqa_model = LLaVA(device=torch.device(\"cuda\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/pathin/safety_llm/Trust-Medical-LVLM/temp/images/VizWiz_v2_000000044696.png\"\n",
    "question = \"what is the image about?\"\n",
    "gt_answer = \"\"\n",
    "prompt_idx=4\n",
    "# vqa_model.ask(image_path=image_path, \n",
    "#               question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prefix = load_prompt(question, prompt_idx)\n",
    "prefix_tokens = vqa_model.tokenizer(prefix)\n",
    "start_loc = len(prefix_tokens.input_ids)\n",
    "candidate_scores = []  # pred scores of candidates\n",
    "raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "images = vqa_model.image_processor(raw_image, return_tensors='pt')['pixel_values']\n",
    "images = images.to(vqa_model.model.device, dtype=vqa_model.dtype)\n",
    "max_new_tokens = 512\n",
    "prompt = prefix # + \" {}.\".format(candidate)\n",
    "input_ids = vqa_model.tokenizer(prompt).input_ids\n",
    "max_src_len = vqa_model.context_len - max_new_tokens - 8\n",
    "input_ids = input_ids[-max_src_len:]\n",
    "input_ids = torch.as_tensor([input_ids]).to(vqa_model.model.device)\n",
    "out = vqa_model.model(input_ids,use_cache=True,images=images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "\n",
    "    parser.add_argument(\"--dataset_path\", type=str, default='/path/to/datset')\n",
    "    parser.add_argument(\"--answer_path\", type=str, default=\"output_res\")\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def run(args):\n",
    "    model_type = 'llava'\n",
    "    vqa_model = LLaVA(device=torch.device(\"cuda\")) \n",
    "    \n",
    "    answer_path = f'{args.answer_path}/{model_type}'\n",
    "    os.makedirs(answer_path, exist_ok=True)\n",
    "    sub_dataset = args.dataset_path\n",
    "    test(vqa_model, dataset=sub_dataset, model_type=model_type, prompt_idx=4, save_path=answer_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    run(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safety_llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
