{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/pathin/conda_envs/safety_llm_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import List\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "\n",
    "from src.utils.registry import registry\n",
    "from src.utils.utils import get_abs_path\n",
    "from src.models.llava.model.builder import load_pretrained_model\n",
    "from src.models.llava.eval.run_llava import chat_model\n",
    "\n",
    "from src.models.base import BaseChat, Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @registry.register_chatmodel()\n",
    "class LLaVAChat(BaseChat):\n",
    "    \"\"\"\n",
    "    Chat class for llava model,\n",
    "    the specific version is LLaVA-1.5 from https://github.com/haotian-liu/LLaVA.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: update model config\n",
    "    MODEL_CONFIG = {\n",
    "        \"llava-v1.5-7b\": 'configs/models/llava/llava-1.5-7b.yaml',\n",
    "        \"llava-v1.5-13b\": 'configs/models/llava/llava-1.5-13b.yaml',\n",
    "        \"llava-v1.6-13b\": 'configs/models/llava/llava-1.6-13b.yaml',\n",
    "        \"LVIS-Instruct4V\": 'configs/models/llava/LVIS-Instruct4V-Nodetail-mix619k-7b.yaml',\n",
    "    }\n",
    "    model_family = list(MODEL_CONFIG.keys())\n",
    "\n",
    "    def __init__(self, model_id: str, device: str=\"cuda:0\"):\n",
    "        super().__init__(model_id)\n",
    "        self.device = device\n",
    "        config = self.MODEL_CONFIG[self.model_id]\n",
    "        self.config = OmegaConf.load(get_abs_path(config))\n",
    "\n",
    "        self.model_name = self.config.model.model_name\n",
    "        model_path = self.config.model.model_path\n",
    "\n",
    "        self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\n",
    "            model_path=model_path,\n",
    "            model_base=None,\n",
    "            model_name=self.model_name,    # get_model_name_from_path(model_path)\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def chat(self, messages: List, **generation_kwargs):\n",
    "        # TODO: if system message provided.\n",
    "        assert len(messages) == 1, 'Only support one-turn conversation currently'\n",
    "        for message in messages:\n",
    "            if message[\"role\"] in [\"system\", \"user\", \"assistant\"]:\n",
    "                if message[\"role\"] == \"user\":\n",
    "                    if isinstance(message[\"content\"], dict):\n",
    "                        # multimodal\n",
    "                        image_path = message[\"content\"][\"image_path\"]\n",
    "                        user_message = message[\"content\"][\"text\"]\n",
    "                    else:\n",
    "                        image_path = None\n",
    "                        user_message = message[\"content\"]\n",
    "                elif message[\"role\"] == \"assistant\":\n",
    "                    # TODO: add assistant answer into the conversation\n",
    "                    pass\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported role. Only system, user and assistant are supported.\")\n",
    "\n",
    "        if generation_kwargs.get(\"do_sample\") == False:\n",
    "            temperature = 0.0\n",
    "            top_p = 1.0\n",
    "        else:\n",
    "            # TODO: set the default value\n",
    "            temperature = generation_kwargs.get(\"temperature\", 0.0)\n",
    "            top_p = generation_kwargs.get(\"top_p\", 1.0)\n",
    "\n",
    "        args = type('Args', (), {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"query\": user_message,\n",
    "            \"conv_mode\": None,\n",
    "            \"image_file\": image_path,\n",
    "            \"sep\": \",\",\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"num_beams\": 1,\n",
    "            \"max_new_tokens\": generation_kwargs.get(\"max_new_tokens\", 256),\n",
    "            'dtype': torch.float16\n",
    "        })()\n",
    "\n",
    "        output_text = chat_model(self.tokenizer, self.model, self.image_processor, args)\n",
    "        # print(output_text)\n",
    "        scores = None\n",
    "        \n",
    "        return Response(self.model_id, output_text, scores, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/pathin/conda_envs/safety_llm_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data/pathin/conda_envs/safety_llm_env/lib/python3.10/site-packages/transformers/modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.19s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "CLIPVisionModel does not support `device_map='auto'` yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLLaVAChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mllava-v1.5-7b\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mLLaVAChat.__init__\u001b[0;34m(self, model_id, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel_name\n\u001b[1;32m     24\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel_path\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_len \u001b[38;5;241m=\u001b[39m \u001b[43mload_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# get_model_name_from_path(model_path)\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tmp_llm/Trust-Medical-LVLM/notebooks/../src/models/llava/model/builder.py:157\u001b[0m, in \u001b[0;36mload_pretrained_model\u001b[0;34m(model_path, model_base, model_name, load_8bit, load_4bit, device_map, device, use_flash_attn, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m vision_tower \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_vision_tower()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vision_tower\u001b[38;5;241m.\u001b[39mis_loaded:\n\u001b[0;32m--> 157\u001b[0m     \u001b[43mvision_tower\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    159\u001b[0m     vision_tower\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice_map, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n",
      "File \u001b[0;32m~/tmp_llm/Trust-Medical-LVLM/notebooks/../src/models/llava/model/multimodal_encoder/clip_encoder.py:30\u001b[0m, in \u001b[0;36mCLIPVisionTower.load_model\u001b[0;34m(self, device_map)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor \u001b[38;5;241m=\u001b[39m CLIPImageProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_tower_name)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_tower \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPVisionModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_tower_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_tower\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/data/pathin/conda_envs/safety_llm_env/lib/python3.10/site-packages/transformers/modeling_utils.py:2703\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2694\u001b[0m special_dtypes\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m   2695\u001b[0m     {\n\u001b[1;32m   2696\u001b[0m         name: torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2699\u001b[0m     }\n\u001b[1;32m   2700\u001b[0m )\n\u001b[1;32m   2702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_no_split_modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not support `device_map=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_map\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2704\u001b[0m no_split_modules \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_no_split_modules\n\u001b[1;32m   2705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_low_0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequential\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[0;31mValueError\u001b[0m: CLIPVisionModel does not support `device_map='auto'` yet."
     ]
    }
   ],
   "source": [
    "model = LLaVAChat(model_id='llava-v1.5-7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safety_llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
